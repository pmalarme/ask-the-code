{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Indexing\n",
    "\n",
    "This notebook is used to index the code in a vector store/database.\n",
    "\n",
    "## 1. Setup OpenAI API\n",
    "\n",
    "First we setup the OpenAI API and embeddings.\n",
    "\n",
    ">The max retries is increased to 50 to be sure that if OpenAI API is throttling us to several calls per minute, we can still get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.redis import Redis\n",
    "\n",
    "API_KEY = os.getenv('AZURE_OPENAI_API_KEY') \n",
    "RESOURCE_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "openai.api_type = 'azure'\n",
    "openai.api_key = API_KEY if API_KEY else ''\n",
    "openai.api_base = RESOURCE_ENDPOINT if RESOURCE_ENDPOINT else 'https://<your-api-name>.openai.azure.com'\n",
    "openai.api_version = '2023-03-15-preview'\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version\n",
    "\n",
    "embeddings_model_name = 'text-embedding-ada-002'\n",
    "embeddings = OpenAIEmbeddings(model=embeddings_model_name, chunk_size=1, max_retries=50)\n",
    "\n",
    "# The GPT model name is used for testing purpose only\n",
    "gpt_model_name = '<Add your gpt model name here>'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Index the code\n",
    "\n",
    "We index the code in the vector store.\n",
    "\n",
    "### 3.1. Setup the indexing\n",
    "\n",
    "We setup the vector store where the code will be loaded, the code folder to be indexed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the index that contains the embeddings\n",
    "index_name = \"<insert index name>\"\n",
    "# URL of the Redis Stack\n",
    "index_url = \"redis://<login>:<password>@<host>:6379\"\n",
    "\n",
    "# Directories where the code is located\n",
    "code_root_directory = 'outputs/<add your path here>'\n",
    "\n",
    "# The keywords that represents folders to be ignored\n",
    "ignore_paths = [\n",
    "    '.git',\n",
    "    '.vscode',\n",
    "    'target',\n",
    "    'node_modules',\n",
    "    'build'\n",
    "]\n",
    "\n",
    "# The keywords that represents files to be ignored\n",
    "ignore_files = [\n",
    "    '.gitignore'\n",
    "]\n",
    "\n",
    "# The keywords that represents file extensions to be ignored\n",
    "ignore_extensions = [\n",
    "    '.class'\n",
    "]\n",
    "\n",
    "# The size of the chunks\n",
    "chunk_size = 2000\n",
    "\n",
    "# The overlap between chunks\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Common functions\n",
    "\n",
    "The functions below are used to check if the path or the file must be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_path_ignored(path):\n",
    "    \"\"\"Checks if the path is in the ignore list or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path to be checked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the path is in the ignore list, False otherwise.\n",
    "    \"\"\"\n",
    "    for ignore_path in ignore_paths:\n",
    "        if ignore_path in path:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_file_ignored(filename):\n",
    "    \"\"\"Checks if the file is in the ignore list or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to be checked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file name is in the ignore list, False otherwise.\n",
    "    \"\"\"\n",
    "    for ignore_file in ignore_files:\n",
    "        if filename == ignore_file:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_extension_ignored(filename):\n",
    "    \"\"\"Checks if the file extension is in the ignore list or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to be checked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file extension is in the ignore list, False otherwise.\n",
    "    \"\"\"\n",
    "    for ignore_extension in ignore_extensions:\n",
    "        if filename.endswith(ignore_extension):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Get the documents\n",
    "\n",
    "Use a text loader to get the documents from the code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(code_root_directory):\n",
    "    if is_path_ignored(dirpath):\n",
    "        continue\n",
    "    for filename in filenames:\n",
    "        if is_file_ignored(filename) or is_extension_ignored(filename):\n",
    "            continue\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load_and_split()\n",
    "            for document in documents:\n",
    "                document.metadata = {\n",
    "                    \"source\": f\"{dirpath}/{filename}\",\n",
    "                    \"fileName\": filename\n",
    "                    }\n",
    "            docs.extend(documents)\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Split the documents in chunks\n",
    "\n",
    "We split the documents in chunks of 2000 lines using the recursive character text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Index the chunks\n",
    "\n",
    "We index the chunks in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis = Redis.from_documents(texts, embeddings, redis_url=index_url,  index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the indexing\n",
    "\n",
    "Test the indexing by using the prompt to ask the purpose of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = redis.as_retriever()\n",
    "retriever.k = 20\n",
    "\n",
    "chat_model = AzureChatOpenAI(deployment_name=gpt_model_name, temperature=0.0, model_kwargs={ 'top_p': 1.0 })\n",
    "qa = ConversationalRetrievalChain.from_llm(chat_model, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "question = 'What is the purpose of the project?'\n",
    "response = qa({'question': question, 'chat_history': []})\n",
    "print(f\"-> **Question**: {question} \\n\")\n",
    "print(f\"**Answer**: {response['answer']} \\n\")\n",
    "sources = response['source_documents']\n",
    "i = 0\n",
    "for source in sources:\n",
    "    if i > 5:\n",
    "        break\n",
    "    i += 1\n",
    "    print(f\"**Source**: {source.metadata} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
